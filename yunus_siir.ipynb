{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3314e751-6fb0-479a-83bc-9fa5f82e0aa8",
   "metadata": {},
   "source": [
    "# Sonraki Kelime Tahmin Modeli\n",
    "Bu model normalde bir metin yazma modeli olarak kullanılmaktadır. \n",
    "Ancak bu projede sonraki kelimeyi değil önceki kelimeyi tahmin ederek şiir yazmaya çalışıyoruz. \n",
    "Çünkü şiirde satır sonlarındaki kelimelerin kafiyeli olması beklenir. \n",
    "Dolayısıyla bir kelime seçip o kelimeyi satır sonuna koyarak önceki kelimeleri tahmin edip bir şiir dizesi oluşturuyoruz. \n",
    "Bir sonraki satırda da yine kafiyeli bir kelimeden yola çıkarak öndeki kelimer sırayla tahmin edilir ve şiir tamamlanır.\n",
    "<br><br>\n",
    "Bu modeli eğitmek için Yunus Emre şiirleri kullanılmıştır. Şiirler başlıksız olarak alt alta eklenip bir metin dosyasına yerleştirildi.\n",
    "### Gerekli kütüphaneleri ekleyip şiirleri okuyarak işe koyulalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8121bf-1df8-4f35-9a67-a9ccede28f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Read the text file\n",
    "with open('yunus.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3957adcc-7ddf-4669-9c2d-6f926e47f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metinde gözden kaçan rakamlar varsa onları temizleyelim\n",
    "import re\n",
    "text=re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e80f60-bc5f-4a9d-a25a-6e3ac987cbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be3c559-9268-447a-b600-ac7be2329843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burada kitabın her bir satırını alıp kelimelerden ngram dizileri oluşturuyor. \n",
    "# Yani satırdaki kelimeleri sayıyay dönüştürüp (Örn:1, 1561, 5, 129, 34) bu sayılardan 2, 3, 4 ve 5 gram lı diziler oluşturuyor.\n",
    "#[1, 1561],\n",
    "#[1, 1561, 5],\n",
    "#[1, 1561, 5, 129],\n",
    "#[1, 1561, 5, 129, 34]\n",
    "\n",
    "# Sonraki kelimeyi tahmin etmek için aşağıdaki kod kullanılmalıdır.\n",
    "#input_sequences = [tokenizer.texts_to_sequences([line])[0][:i+1] for line in text.split('\\n') \n",
    "#                   for i in range(1, len(tokenizer.texts_to_sequences([line])[0]))]\n",
    "\n",
    "# Ancak biz önceki kelimeyi tahmin etmek istediğimiz için sıralamayı ters çeviriyoruz.\n",
    "input_sequences = [tokenizer.texts_to_sequences([line])[0][i::-1] for line in text.split('\\n') \n",
    "                   for i in range(1, len(tokenizer.texts_to_sequences([line])[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a19d9d-257e-4df7-9040-b8bc88a0ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yukarıda oluşturduğumuz input_sequences dizisi farklı boyutlardan oluşan dizi elemanlarına sahip.\n",
    "# Bunlar içerisinde en uzun dizinin kaç elemanı olduğunu bulalım. \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef9cd7c-86d5-492f-b772-6c6ea204a59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,  818, 1424],\n",
       "       [   0,    0,    0, ...,  819,  818, 1424],\n",
       "       [   0,    0,    0, ...,  819,  818, 1424],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0,   80, 1422],\n",
       "       [   0,    0,    0, ..., 1423,   80, 1422],\n",
       "       [   0,    0,    0, ..., 1423,   80, 1422]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yukarıda oluşturduğumuz input_sequences dizisinin eleman sayılarını eşitliyoruz. \n",
    "# Bunu yaparken baş kısımlara (padding='pre') 0 değerleri atanır.\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4de571-24ca-445e-a822-602bf404765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oluşturacağımız modelde girdi değerlerimiz oluşturduğumuz input_sequences dizinin her bir satırının ilk 15 elemanı\n",
    "# Çıktı değişkenimiz ise bu dizinin 16. elemanları\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f4ee60-1d7d-40e6-87d2-14866fdf4396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Burada hedef değişkeni one-hot kodlamasına dönüştürüyoruz. \n",
    "# Bu, her hedef değeri için bir vektör oluşturur ve hedef değerinin indeksine karşılık gelen yerde 1 ve diğer tüm yerlerde 0 içerir.\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38f892-7efa-470a-a294-5ee7f3c5232d",
   "metadata": {},
   "source": [
    "### Modelleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b9aaa9-be7e-4049-89c8-f89fdd1bf7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_shape=(max_sequence_len-1,)))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e05cc1-6e80-44a4-97b4-4a845f506595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.0394 - loss: 7.0914\n",
      "Epoch 2/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.0485 - loss: 5.7974\n",
      "Epoch 3/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.0568 - loss: 5.5084\n",
      "Epoch 4/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.0742 - loss: 5.1918\n",
      "Epoch 5/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.1079 - loss: 4.9103\n",
      "Epoch 6/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.1475 - loss: 4.5993\n",
      "Epoch 7/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.1917 - loss: 4.2773\n",
      "Epoch 8/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.2630 - loss: 3.8985\n",
      "Epoch 9/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.3461 - loss: 3.4892\n",
      "Epoch 10/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4235 - loss: 3.0769\n",
      "Epoch 11/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5041 - loss: 2.6870\n",
      "Epoch 12/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5894 - loss: 2.2913\n",
      "Epoch 13/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6662 - loss: 1.9330\n",
      "Epoch 14/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7255 - loss: 1.6108\n",
      "Epoch 15/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7742 - loss: 1.3331\n",
      "Epoch 16/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8171 - loss: 1.1240\n",
      "Epoch 17/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8628 - loss: 0.9162\n",
      "Epoch 18/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8889 - loss: 0.7477\n",
      "Epoch 19/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8956 - loss: 0.6365\n",
      "Epoch 20/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9146 - loss: 0.5160\n",
      "Epoch 21/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9167 - loss: 0.4394\n",
      "Epoch 22/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9230 - loss: 0.3820\n",
      "Epoch 23/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9238 - loss: 0.3480\n",
      "Epoch 24/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9275 - loss: 0.3047\n",
      "Epoch 25/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9248 - loss: 0.2928\n",
      "Epoch 26/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9229 - loss: 0.2733\n",
      "Epoch 27/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9278 - loss: 0.2513\n",
      "Epoch 28/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9245 - loss: 0.2511\n",
      "Epoch 29/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9288 - loss: 0.2272\n",
      "Epoch 30/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9259 - loss: 0.2276\n",
      "Epoch 31/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9246 - loss: 0.2194\n",
      "Epoch 32/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9298 - loss: 0.2030\n",
      "Epoch 33/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9258 - loss: 0.2153\n",
      "Epoch 34/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9208 - loss: 0.2144\n",
      "Epoch 35/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9273 - loss: 0.2049\n",
      "Epoch 36/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9227 - loss: 0.1997\n",
      "Epoch 37/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9271 - loss: 0.1915\n",
      "Epoch 38/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9290 - loss: 0.1879\n",
      "Epoch 39/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9276 - loss: 0.1878\n",
      "Epoch 40/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9267 - loss: 0.1833\n",
      "Epoch 41/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9265 - loss: 0.1860\n",
      "Epoch 42/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9255 - loss: 0.1989\n",
      "Epoch 43/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9268 - loss: 0.1795\n",
      "Epoch 44/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9253 - loss: 0.1865\n",
      "Epoch 45/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9289 - loss: 0.1788\n",
      "Epoch 46/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9260 - loss: 0.1834\n",
      "Epoch 47/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9254 - loss: 0.1825\n",
      "Epoch 48/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9268 - loss: 0.1779\n",
      "Epoch 49/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9287 - loss: 0.1752\n",
      "Epoch 50/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9242 - loss: 0.1798\n",
      "Epoch 51/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9260 - loss: 0.1791\n",
      "Epoch 52/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9255 - loss: 0.1808\n",
      "Epoch 53/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9285 - loss: 0.1771\n",
      "Epoch 54/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9263 - loss: 0.1733\n",
      "Epoch 55/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9250 - loss: 0.1778\n",
      "Epoch 56/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9301 - loss: 0.1649\n",
      "Epoch 57/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9278 - loss: 0.1739\n",
      "Epoch 58/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9229 - loss: 0.1855\n",
      "Epoch 59/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9323 - loss: 0.1625\n",
      "Epoch 60/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9248 - loss: 0.1787\n",
      "Epoch 61/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9279 - loss: 0.1746\n",
      "Epoch 62/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9301 - loss: 0.1550\n",
      "Epoch 63/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9336 - loss: 0.1553\n",
      "Epoch 64/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9251 - loss: 0.1748\n",
      "Epoch 65/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9327 - loss: 0.1622\n",
      "Epoch 66/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9287 - loss: 0.1665\n",
      "Epoch 67/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9263 - loss: 0.1710\n",
      "Epoch 68/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9255 - loss: 0.1709\n",
      "Epoch 69/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9276 - loss: 0.1723\n",
      "Epoch 70/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9268 - loss: 0.1692\n",
      "Epoch 71/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9279 - loss: 0.1633\n",
      "Epoch 72/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9326 - loss: 0.1572\n",
      "Epoch 73/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9328 - loss: 0.1573\n",
      "Epoch 74/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9357 - loss: 0.1514\n",
      "Epoch 75/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9312 - loss: 0.1605\n",
      "Epoch 76/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9338 - loss: 0.1501\n",
      "Epoch 77/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9279 - loss: 0.1651\n",
      "Epoch 78/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9281 - loss: 0.1683\n",
      "Epoch 79/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9271 - loss: 0.1644\n",
      "Epoch 80/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9293 - loss: 0.1621\n",
      "Epoch 81/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9267 - loss: 0.1676\n",
      "Epoch 82/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9255 - loss: 0.1641\n",
      "Epoch 83/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9287 - loss: 0.1640\n",
      "Epoch 84/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9273 - loss: 0.1638\n",
      "Epoch 85/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9290 - loss: 0.1708\n",
      "Epoch 86/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9304 - loss: 0.1547\n",
      "Epoch 87/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9229 - loss: 0.1706\n",
      "Epoch 88/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9307 - loss: 0.1585\n",
      "Epoch 89/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9259 - loss: 0.1683\n",
      "Epoch 90/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9288 - loss: 0.1606\n",
      "Epoch 91/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9295 - loss: 0.1632\n",
      "Epoch 92/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9284 - loss: 0.1657\n",
      "Epoch 93/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9287 - loss: 0.1660\n",
      "Epoch 94/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9327 - loss: 0.1532\n",
      "Epoch 95/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9282 - loss: 0.1678\n",
      "Epoch 96/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9283 - loss: 0.1614\n",
      "Epoch 97/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9304 - loss: 0.1524\n",
      "Epoch 98/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9322 - loss: 0.1587\n",
      "Epoch 99/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9271 - loss: 0.1670\n",
      "Epoch 100/100\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9305 - loss: 0.1555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26e240ff750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ac916-600e-4de2-9785-af4e69cd7b81",
   "metadata": {},
   "source": [
    "### Sonuç:\n",
    "Geliştirilen modelde Yunus Emre şiirlerini taklit eden ve satır sonunda başlayarak bir önceki kelimeyi tahmin eden bir araç geliştirilmiştir. \n",
    "<br><br>Modeli kaydedip streamlit uygulamasına dönüştürelim. Uygulamada kullanılmak üzere tokenizer nesnesi ve bu nesnede kullanılmak üzere kelime-index sözlüğü kaydedilmelidir. Tek başına model yeterli değildir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738ed489-c4bd-437b-b3e1-6a7f1693b314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Modelin kaydedilmesi\n",
    "model.save('yunus_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cef847d-7bf1-4b62-b9d2-748564467039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer nesnesinin kaydedilmesi\n",
    "import pickle\n",
    "pickle.dump(tokenizer, open(\"yunus_tokenizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "099ec46d-f46f-483b-b119-70ad4123ccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4603 entries, 0 to 4602\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   word    4603 non-null   object\n",
      " 1   index   4603 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 72.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Kelime-index sözlüğünün kaydedilmesi\n",
    "import pandas as pd\n",
    "word_index_df = pd.DataFrame(list(tokenizer.word_index.items()), columns=['word', 'index'])\n",
    "# Sözlükte rakam istemiyorsanız aşağıdaki satırı kullanabilirsiniz.\n",
    "word_index_df = word_index_df[~word_index_df[\"word\"].str.isnumeric()]\n",
    "word_index_df.to_csv('yunus_word_index.csv', index=False)\n",
    "word_index_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aeae57-9283-442a-b744-74497f5186a2",
   "metadata": {},
   "source": [
    "### Test Edelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e92f47e-345d-439a-8980-67155bb0289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "word_index_df = word_index_df[~word_index_df[\"word\"].str.isnumeric()]\n",
    "\n",
    "def kafiye(kelime):\n",
    "    k=random.randint(1, 2)\n",
    "    return word_index_df[word_index_df[\"word\"].str.endswith(kelime[-k:])].sample().iloc[0][\"word\"]\n",
    "\n",
    "def pre_words(seed_text, n):\n",
    "    for _ in range(n):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a953e-8d66-4206-9cf0-b60efe9d0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "onceki=\"\"\n",
    "next=\"dünya\"\n",
    "siir=[]\n",
    "while(i<13):\n",
    "    dize=pre_words(next, 5)\n",
    "    next=kafiye(dize.split()[0])\n",
    "    if(onceki!=dize):\n",
    "        siir.append((\" \".join(dize.split()[::-1])).capitalize())\n",
    "        onceki=dize\n",
    "        i=i+1\n",
    "    if((i-1)%4==0):\n",
    "        siir.appen(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
